---
layout: post
title: 逻辑回归——机器学习笔记（三）
date: 2014-8-17 01:21
comments: true
reward: true
tags:
    - Algorithm
---

逻辑回归(Logistic regression)是一种二值型输出分类器，其基本思路是对于数据的边界线建立回归公式，以此进行分类。

比如，银行在处理信用卡申请时，需要判断申请人是否有能力使用信用卡。根据申请人的年龄，职业，信用记录等信息，以此作为输入数据点x，来预测申请人是否有能力使用信用卡，即输出1或0表示是否。

<!-- more -->
    
    最近入手了MacBook，于是就把学习MachineLearning以及Coding的环境换到了OS X（不得不说MacBook的用户体验简直太nice了）。
    
    之后就是学习了一下NumPy，简直是太好用了...于是决定以后在学习MachineLearning的时候，用C++敲算法，之后用python来看效果。

为了实现接受输入然后推测输出，并且输出值只能是0或1。于是需要一个函数来实现分类。Sigmoid函数是一种阶跃函数，并且易于处理：

$g(z)=\frac{1}{1+e^z}$

要实现这个分类器，需要把输入的每个特征乘上回归系数 $\theta$，把结果加起来，即$\theta^Tx$代入Sigmoid函数，构造出hypothesis函数。

$h_\theta(x)=\frac{1}{1+e^{-\theta Tx}}$

根据最大似然分析，可知
似然性


$$L(\theta)=\prod^{m}(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}$$

先对等式两边取对数。

$logL(\theta)=l(\theta)=\sum^m_{i=1}y^{(i)}logh(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))$

要使似然性最大，可以用类似之前线性回归的方法对$l(\theta)$求导数。

$\frac{∂}{∂\theta_j}l(\theta)=(y-h_ \theta(x))x_j$

化简得到算法
$\theta_j := \theta_j + \alpha(y^{(i)}-h _\theta(x^{(i)}))x_j^{(i)}$

因为这里其实是在求最大值，所以这里的算法应该是叫梯度上升算法。

我的代码实现使用的是随机梯度上升。

最后是用NumPy库展示出的效果图。

<img src="http://bcs.duapp.com/blogbuk/logistic_regression.png"/>

[C++代码实现][1]

[1]:https://github.com/plusplus7/MachineLearning/blob/master/src/logistic_regression/logistic_regression.cc
