# 局部加权线性回归——机器学习笔记（二）

---

局部加权回归（Locally weighted linear regression）在进行预测时，都需要基于训练集来重新拟合一条曲线。

在$J(\theta)$函数中增加一个$w(i)$作为当前数据点的权值，通过对每个数据点进行加权后使用线性回归算法（如：梯度下降.etc），就能得到一条基于当前预测点的局部加权回归后的预测值。

在进行线性规划的时，有时得到训练模型的预测结果并不能很好地拟合数据集（欠拟合）

。如果尝试给模型添加几个特征值，如：房屋大小$x_1$，房屋面积$x_2$...当添加太多特征值后，最终会训练出一个完美通过每个数据点的曲线。

显然，这样的曲线不能并不能反应数据所表达的普遍特征（过拟合）。

From 《机器学习》P49 Tom M.Mitchell

*给定一个假设空间H，一个假设h属于H，如果存在其他的假设h’属于H,使得在训练样例上h的错误率比h’小，但在整个实例分布上h’比h的错误率小，那么就说假设h过度拟合训练数据。*

局部加权回归的步骤是：

* 检查数据集合，考虑位于x周围的固定区域内的数据点

* 对这个区域内的点做线性回归，拟合出一条直线

* 根据拟合直线对x的输出，作为算法返回的结果

其中权值函数weight的选择：

$w^{(i)}=exp(-\frac{(x^{(i)}-x)^2}{2\tau^2 })$

如果使用批量梯度下降来实习线性回归的话：

$Repeat\ until\ convergence${

$\theta_j:=\theta_j+\alpha\sum_{i=1}^mw^{(i)}×(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)} $   

(for every j)

}

[局部加权回归 C++实现][1]

[1]:https://github.com/plusplus7/MachineLearning/blob/master/src/locally_weighted_linear_regression/locally_weighted_linear_regression.cc